Anyone can build browsers, we have to follow a standard(given by ECMA international) that the browser-building companies have to follow. Companies like Google and Firefox wrote their engines to convert JS code to binary that can then be understood by the machine.  
Native programs- That we can run on local machines. Earlier JS could only be used as web programs, so Node.js written by Google was pulled out and converted to a backend framework. Node.js is not a language or framework, it is just a JS runtime used to run JS in the backend in a local machine outside the Chrome browser.
JS is single-threaded language, which means it is kind of independent of how powerful our machine is, as it will utilize only one thread.  
JS is Asynchronous. Reading from a file, waiting for input from the key, writing to a database, or sending an HTTP request to a backend are all asynchronous tasks, as they can block the code if it is running synchronously. While asynchronous task is running we can do other tasks. (analogy- tell your friend to tell you that 1 hour has passed and now I have to drink water when I am busy studying. So now I don't have to look at the clock and can focus only on studying.)  
If we send a question to chatgpt, then it will try to fetch its answer using the algorithm. This is a long running process, if this was not asynchronous then it will block the main thread of the code and any button click we do on the website will not work
JS is loosely typed, the argument of a function need not be defined with its type.  
Interpreted language, goes line by line. While other languages like C++ will first do Compile checks, in its compilation time, it will give errors if the code has even a single error. (in JS the thread never goes inside a function until it is called, it just registers the function name.)  
APIs- exposed by the runtime environment, for example, fs, fetch, setTimeout, and setInterval are APIs.  
SetTimeout, setInterval takes a callback, and it is an asynchronous function.  
If you know about async JS, callback, Promises, and arrow functions you can sprint much faster.  
JS is asynchronous but it does not mean that it can do 2 tasks at the same time, if the thread is doing a time-consuming task and an asynchronous callback function is called during that  time, the async function will not work as the thread is busy executing the long task at the moment.  
Promises are not needed as such in js, you can do the same thing without them, but js peeps want code to be clean, and using promises you can write cleaner code.
Call stack, web APIs, event loop, callback queue. Web Apis is the friend that does the async task in the background. Once that is done, it puts the callback function in the callback queue, the function then waits there until the call stack is empty and the event loop pulls the function from the callback queue and puts it into the call stack for executing it. http://latentflip.com/loupe Check this website to see how the simulation is done  
Async chaining is done if you want one task to happen before another async task, because the first task is more important, eg, on chatgpt website the name appears before the upgrade plan and our history. Callback hell is this chaining of one async call after another, the code looks ugly due to this if we do more than 3 async calls one inside another.  
Promise returns an object, .then() is a function that is used to call the function we want to execute once the promise is returned(the callback function). The promise function takes resolve as an argument, when resolve is called, the control reaches then, resolve is the first argument of promise function, can be called anything.  
Backend- HTTP, Authentication, Database, Middlewares. Machines communicate using HTTP protocol with each other. Eg, I use servers of chatgpt to get answers to my questions as chatgpt servers run a very heavy ML algorithm, on my light machine which cannot use these algorithms on my local machine or phone. Eg, if you open Instagram, the feed, people's dp, etc comes from the servers/backend as they are not on my browser.  
You need to know where to send the server, there are many servers- servers of Google, facebook, openai, etc. We need to know their backend URL, eg if it is chatgpt, we use HTTP://chat-openai.com. The next part is route which tells what algorithm we want to run example HTTP://chat-openai.com/conversation/ conversation is the route.  
npm init -y initialize package.json file. Express is a library in nodejs that lets us create HTTP server. When you install new libraries, it is shown in the dependencies in package.json file. run node index.js to run the HTTP server.  
A long running process is a process that runs infinitely, like setInterval, or a server. The significance of route is that each route lets us do something different. In fs.readfile, the callback function takes two arguments, err and data, data contains the content of the file. FS is also a library like express is a library, both of nodejs.  
if we want to get input from user, we can do that using- query param, header, body. after the route add ? and then add the params. HTTP request has URL, route, and method. There are many methods, but the important one's are get, put, post and delete. Default is get request. Creating an account on instagram is a post request, updating data use put request, deletin use delete request. It doesn't have to be, it is a standard that is followed, we can do all things with get request, not recommended.  
app.get('/:username') this become wildcard, any route will now be able to do get request.it can be extracted as req.params.username. It is hard to send post request from the browser so initially use postman. Through url we can only send get request by default.  
IRL we use body to send data, 95% of the time. But first, we should see what is Middlewares. If we introduce a middleware then any request first go to middleware, which does some logic like authentication check and then the control reaches the other functions. Middleware in nodejs takes 3 argument- req,res and next, next is the next handler that we want to use if the request passes the logic of middleware. Have to write app.use(middleware1) to tell express to register the middleware so that all the requests are first sent to the middleware. Call next() in the middleware1 function which automatically calls the function for that route.   Middleware can also send res using res.send('hi') if we don't want to call the next function. If we try to send res.send() and also call next() which also contains res.send() then it will give error. Middleware is a function which is called before any route handler is called for any request. Middleware also helps to define a logic that we want to use in every route handler  
Body is something that express don't give us out of the box as like query params and headers. req.body is nothing as of req.query or req.headers. Express is a generic server, so you will bring xml,text,json,etc formats, they don''t want to provide support of all of them, that is why we have to use external libraries built by people called body-parser. npm install body-parser. app.use(bodyParser.json()). this bodyparser.json is a external middleware that we are using. bodyparser.json is a new middleware, which is just a funcition that extracts and parses the json in the body in the background, whose procedure we don't need to know. now we can use req.body().  
query params, headers and body are the three ways to send data. The function within the app.use() is the middleware. We can define route specific middleware as well can read more about them.  
the server responds with status code, body, headers. Status code are defined and are numbers like 404-not found page. The number can be anywhere between 100-600. No need to learn them. if 100-199 informational response, 200-299 succuessful response, mozilla page is the best page to read about each code. Express default send 200 status code. res.status(401).send(answer) will send 401 status code. 400-499 client error response, 500-599 server error. We can make a website only returning 200 code but that is not recommended. If our backend code is errornous then server error is returned.  
Response body can be of 3 types html, json, simple text. res.send('hi') is simple text type that is a string. Most of the times json is sent not simple text, as we can send more things in a simpler way. res.json() and res.send() are same, but we cannot send anyother type if we use res.json() expect json. If we send html code in the res.send() it will render on the page. this is for a get request. res.sendFile(__dirname+"/index.html") is used to directly  send the code from the html file. we cannot send res.send() multiple times.  
till now we saw that browsers and postman can send req to http servers, we can send req using nodejs process as well. It is done using another external library called fetch. so fetch function expects 3 arguments just in pattern of readFile function, the url we want to hit, the method type, the callbackFunc we want to call after successful connection with the url. Fetch  returns a promise so the callbackFunc is called using the .then() syntax. To get the json from the result we use result.json().then(callbackfunc) as .json() returns a promise so we have to use .then() see the code in the secondProcess.js to get clear idea as it is not that clear from here.  
Database- we can store data in json object in the server file locally but if the process restarts or computer restarts then the data is lost as it resets to empty as in the starting, so we can put the data in a separate file by reading and writing from that file as a temporary replacement. Whenever we read from a file we always get string, even the json is of string type, does not look good so we have to parse it in json type. we call JSON.parse(data), where data is the string we get from the file. Arrays are also JSON objects. Json.parse and json.stringify are opposite of each other. Till now we were using only POSTMAN to utilize our backend, now we have to connect it the frontend of the website so that users can use it via browser and phone.  
Connecting backend to frontend- the code we write in the html file in the script tag for onclick function is mostly client side not server side of express.  
CORS error- what if chatgpt frontend can send request to google backend or google send request to chatgpt backend, using fetch we can send request to anywhere, if I know which url to hit for a companies backend. This is not good for multiple security reasons, hence it is blocked by default. Cross Origin Response Error. First way to deal with this is to on app.get("/") sendFile of the index.html so that the frontend is served from the backend. this way we don't get cors error. The frontend url is as the backend URL, this is what browsers think and then we are ok. The other way to get away with cors error is add, cors=require("cors"), and app.use(cors()) this will tell browser it is ok to get request from anywhere, not recommended in a production ready app.  
In js we import using const express=require("express") kind of syntax. Whenever we have to send body using the fetch library we need to stringify the json object we are sending using JSON.stringify(). Also you need to send a header with the body if it is json type that is "Content-Type": "application/json" which tells the broswer that the body is of json type otherwise it will not work, kinda important.  
Legacy frontends - pre 2014. Foundation for react, what is react doing in the background. We will make dynamic frontend application as done in legacy time using vanilla css, html. What react does under the hood, we can send backend request from multiple places like postman, browser, frontend. Using id of html we can fetch what is input given by the user inside js using var title=document.getElementById("title").value. If we console log title, it will appear in the console of browser not vscode as the html is running on browser, vscode is running the backend code in the terminal.  
Inorder to show the data in the website we have to learn about DOM manipulation. How do you add js variables in html. To insert js variable in html, add a div with a id name, then get that div element in js code and use .innerHTML= js variable this will modify the html and add the js variable there. Whatever is inside a html tag, use .innerHTML to get it. if we use var a=document.createElement("div") creates a div element, but this div is not inserted in the dom. We can add text in it using a.innerHTML="hello". to add it to the mainArea div of html use document.getElementById("mainArea").appendChild(a). now the new div is inserted in the DOM of the webpage. This process was done for the longest time to make website dynamic until react came into the picture. We can give attribute to a tag using the .setAttribute function,         grandchild3.setAttribute("onclick","deleteTodo("+data[i].id+")");  
On facebook, opening chat is appendchild and closing the chat is remove child, this is what react do under the hood and hide the complexity to write all that ourselves, we just have to take care of the state variables that are changing.  
What is reconcilation- process of react or view taking the update of a state by calling appendchild or removechild in the background. How to move from existing state to current state, we should only calculate the differnce or diff, between the current state and updated state instead of rerendering the whole new state because that is very unoptimal. That is why frameworks were introduced to remove this hardwork and just focus on the state variable, the appendchild, removechild, etc will be taken care of on its own.  
Authentication- We can define route specific middleware as well, by passing the middleware function as an argument, app.post() can take infinite number of arguments, the last argument is the callback function which defines its functionality, all the middle arguments are middlewares that are called one by one and are passed using next() function in the middlewares. Sending username and password for every authentication request is bad, it should be sent only for the first time, after that we should send a hashed token that you get when you signup for the first time, this token should be sent instead of username and password. Benefits of token, this token get reset after one hour or so, that is why we sometimes get logged out of a website after sometime that is because the token gets expired for that particular username and password, hence there is not much safety concerns if this token is leaked as in if the username and password of a person is leaked, also the token is encryted by hashing. We will use JWT(json web token) in the header instead of sending username and password, username and password need to be sent only once at login or signup. Only the server can decode this token. JWT let you encrypt or decrypt object or strings.  
Encryption vs Hashing- Encryption can change a simple text to a random string, this string can be decrypted to get the text, supersecret is used to encryt the data, different supersecret give differnt output string, hence the supersecret have to be kept only to ourselves, if people know what the supersecret is then all the authentication will fail on our website.  
Files are not optimal to be used as database, also there is technology defined only for this task of storing data called database, so we will use the database for storing data. --database part  
Reconciler and react- Our first approach is clear the DOM completely and iterate over the content and render all the components, dom operations like appendchild, removechild are expensive hence we want to minimize the number of dom operations. Better way is to compare each new element with the current element present on the screen and find the difference or diff, that is we update, add, or remove elements from the dom, instead of removing all and adding the new elements from the completely. More optimal version is declaring a virtual dom global array in the file and assigning it to existing virtual dom array, then we change the virtual dom array with the new elements, we can now compare the new elements and current elements via the arrya instead of calling the dom elements to get the innerhtml and then comparing to update them, this save some time. virtual dom is a copy to the dom in variables of string ,int,etc. React use this virtual dom, which is a big object, and maintains it to calculate the diff and apply the changes.  
In complex frontend apps the state changes quickly, react batch multiple updates, if too many state changes are happening quickly. so it apply these changes together, this is good because if after bunch of updates, you get the same state, then there is no need to update anything. for example state update is happening at 1ms, then we update the dom after 10ms. This is called batch updates that react does beside many other things.  
States and Components- We wrote a reconciler, but our reconciler hard coded the logic of how the thing looks, react or any library to create a website should give independence of how the thing looks to the developer's will, hence it should not take only 1 input that is state, but 2 inputs a state and the component, i.e how the component looks.  
js vs jsx- write html in js file, we use jsx files. App.jsx is the entry point of the website. We can write js code in the html part inside {}, and it will render in the page. Instead of doing document.createElement("div") then var.appendchild(), that would put things to the dom, now we just have to write dynamic js variables inside {}.  
in react there is a certain way to define state variables. we can have 1000 variables in our app, react won't observe all of them, this would make it very slow, so we define the variables that will change in a certain way so that we tell react this is a state variable, observe it, this is not a state variable, don't observe it. use useState to declare a state variable. A component is a functions. Components can take in states called as props which are just the arguments of that function. We can send the arguments within the tag. The props is the only thing we need to give to the component, but we can pass multiple arguments in the html tag, like firstname and lastname, and access them in the component via props.firstname and props.lastname. .map() is a function of js not react, can use it on any array, the input it expects is a function, the function take value one by one as argument. React does not know how to render a json object, so we stringify it {JSON.stringify(todos)}. whenever you need to write js in html use {}. In react component take only a single argument called props, the keys of the props are arguments we pass.  
How rendering happens in react ans useEffect- The setInterval call happened more than once even after the timer is set to 1second. It is due to setInterval is getting called more than once, again and again. WHY? if we remove setInterval, then the App function ran only once. Whenever a state variable changes, reconciler need to re-render which means remove, add and update things from the dom. Hence the App function re runs and the setInterval runs again. React.useState() protects the variable to get its initial value during its re-render. Anytime we updtate the state using set, the App() function is called again.  
Introducing useEffect- usestate() is a hook, hook is a thing that remains independent to a re-render.Hooks/useState protect variables across re-renders. useEffect is a hook, a function like useState(), take 2 arguments, 1st arg is a function and 2nd arg is an array. The login inside the 1st arg function is protected and does not run on re-renders. so useeffect is hook that guards logic from running on re-renders, to avoid infinite loops, etc. If you want to use a js function only once and not on every re-render, then wrap it in useeffect(). fetch() is written in useEffect() for the same reason.  
Custom hooks- like we have used functions from other libraries like fs, fetch, but we can define our custom functions, similarly we can define our custom hooks apart from pre-defined hooks like usestate() or useeffect(), every hook we define needs to start with use. The benefit is that we can encapsulate the logic, any function that uses hooks inside it and returns a value related to hook is a custom hook.  
Components can be thought of as defining our own html tags. We can write css in {{}} double curly braces, like <div style={{background:"red"}}>hi there</div> in react. In the industry we don't write whole css on our own, we use styling frameworks like mui, which have components written already, by other people, we just use it. Google also uses mui. css width=100vw means 100% window width, 100vh means 100% window height. Flex box is used to align elements side by side. div take full width, 2 divs will appear one below the other. flex is applied only to the child elements, not grandchild, we will have to define child with flex if we want grandchild to appear side by side.  
Routing in react- react-router-dom is the library used by majority of the companies.<Route path="/login" element={<signin />}/> means if url route is /login then render signin component. so it is conditionally rendering things. Window.location("/signup") can be used to swith the url to a new page, but this reloads the page, not recommended, we can use Link tag that takes href. The most popular way to get the value that user write on the website is to store them in state variables, like we have forms that will get input from the user. If there is a variable that we want to show in the website and that is changing then we should define it as state variable. the TextField component of mui have onChange attribute that accept a function with argument e, onchang={(e)=>{setUsername(e.target.value)}}, e.target.value gives the value that is currently in the text field. This is the correct way to define and use state variables, or the variables that change in the lifecycle of the website. e.target and document.getelementbyid("id name") is the samething, e.target.value give the value inside the component, similar to document.getelementid("").value. here we don't need  to provide the id name when using e.target version, the mui already knows what we are refering to as the onchange function is the component's argument. The user when sign's up or signin the token he gets need to be stored in the localstorage of the browser so that he comes again he is already signed in, this is done using localStorage.setItem("token", token), can see if this works by inspecting the application->localstorage in inspect of browsers. we can send the authorization and the corresponding bearer token by writing "Authorization": "Bearer"+ localStorage.getItem("token") in the headers json object argument in fetch function, as earlier we were sending this in the postman's header field.  
We want to conditionally render the appbar, signup signin button if not signed in and the logout and user profile if signed in. We can use if else statement to return the component conditionally. When user clicks logout we assign token to null, hence the user is logged out.  
Prop drilling and recoil- React developer tools is an extension used to see the re-renders that happen when we do something on the website. Many times it happens that we have parent child that have 2 grandchild, if both of the grandchild need  a state variable then we have to define it to the first common ancester of the grandchild, as props or state variables can only be passed down, they cannot be passed up. Hence the state variable is defined in the top level parent common to both the elements using it. Hence when the variable(count in our case), is updated anytime, every element re-renders. We can use context to very slightly optimise it. We want re-render to happen only of the element that is showing the state variable not all the elements that are involved in the prop drilling. Context api(createContext) can be used to optimise this, declare it globally so that we define our state outside the top level app so that we can directly use the state variables we were drilling to any element we want(called using useContext). But the number of extra renders, this only help with us passing the prop through all the components, to now defining it in the top element and straightaway calling it where we need it. Recoil is used to prevent extra renders, wrap the app with recoilRoot, define an atom( atom is the state, defined outside the component tree, atom is a state variable. Now what are the places where we need to get the value of variable, so use useRecoilValue(atomname) ) setCount doesn't nessecarily need count to update it, we can change it using this syntax as well, setCount((existingCount)=>existingCount+1), so use this to avoid using count variable as this way react thinks we don't need count value and hence it will not re-render that component that is updating the atom's/state variable value. setCount= useSetRecoilState(atomname). Doing this only render happen of the component that is changing its appearence if the state variable is changed. This is used in large apps or gaming companies where there is heavy frontend work happening.  
If you are asked to make a new page, then find where the routes are defined and introduce the page. react-router-dom is not available in nextjs it is only available in react.  
Backend file structuring- we should create files to store our code in smaller files. Not every thing in one file. There is a top level index.js file, all the routes are moved to different file. Route for /admin are from admin.js and /user is from user.js. Another folder for db, have index.js for it which have schema defined. Object destructuring can be done based on the key. Like if a file exports an object we can import a key of it using object destructuring like import {User} = require('../db'). There is middleware folder, it contains our own defined middleware like authenticateJwt and Secret of the JWT. If the file returns export defualt User. then we don't do destructuring as it is not sending an object, we use const User = require('./admin'). If we have 100 lines in a file or more it is not considered good. To run the backend run the file that contains app.listen(3000), which in this case is top level index.js.  
Axios- Library similar to fetch that is used to send http requests. axios.post() take 2 arguments, first is the url we want to fetch or hit, 2nd is the body(json format). We don't mention the method as in fetch, we just call the function related like axios.post/.put(), etc. Axios does the same thing in a much cleaner and short code way. Async await syntax is better and cleaner way to write promises, instead of .then() syntax, to convert .then to await we assign the promise returning function to a variable( this variable works as the argument of the first callback if it was .then() syntax, eg response), and then use that variable when the await request is suceeded to do the things we used to do in the callback function, now below the await code. The first argument to a useEffect cannot be an async function, so we will have to use callback method for fetching via axios. Or if we have to use async await syntax then we can define another function inside the first argument function which is an async await function and call it at the end.
In mui each page is divided into 12 sections of equal width, these are used as the scale for grid, which element takes how much %age, for example we can do left element with 8 and right element with 4, for lg, for md we can do 12 for left element and 12 for right element, in this case they will appear one below the other as both are taking full width.  
Ok so till now what we have studied is 80% of a normal website. State management libraries store the state variables in a separate file and provide these statte variable to only those components that need it, so we don't have to pass it through prop drilling and render other non related components. To use Recoil we have to wrap our app with RecoilRoot first, any component that is inside recoilRoot can use recoil functions, other components cannot use recoil functions. Next is we have to define a state variable, we do it using creating atom(recoil terminalogy). git reset --hard let you change the local changes to the last commit state for all files. git pull origin master cannot work until you have local changes. git reset --hard <hashcode>, this will bring the HEAD to the commit with <hashcode>, git push origin HEAD, now we have got the code of the commit with <hashcode>. useParams is provided by react-router-dom library to get the params we pass through a URL like /courses/:courseId, here courseId is a param and we can get it using useParams().  
To change the value of state variable we need to get the setUser (an eg) function, we get it using useSetRecoilState(atom name), this will return the setUser function that can be then used to change the value of the state. const setUser= useSetRecoilState(userState). setUser("changed value example"). Whenever we change the value recoil will not re-render, it will only re-render components that show the value of it, not setting it. To get the value we use useRecoilValue  
Sometimes atom have 2 or more state variables defined in json, we can use selectors or subatoms that helps us get each of the state variables separately so we don't need to get the state variable via atom as it will give all the state variable defined in it and if we only want to update one of the state variable, it will update the other state variable defined in it as well. so now the component will re-render only if the part of the atom changes, not the whole atom change. Selectors help when lets say that a component need only the 1st state variable defined in the atom json, if we don't use selector we will have to get the 1st state variable through accessing atom  which will have other state variables as well, if any of the other state variables changes in some other component, then this component will also re-render even when it is not needing the state variable that changed, hence we get the specific 1st state variable using selectors, which let us break the atom into subatoms. In real world projects developers don't optimise this much hence you can provide value by fixing this on any project, they usually get the full store from atom instead of using selectors, hence you can provide value there to optimise the project. This is the correct way to optimise any website in terms of re-renders  
Why Typescript- ts is very hard than js, it expects you to write more for the same thing, also js is not used in the industry. Ts is a superscript of js, if we create a .ts file we can write js in that file, it just have more things that we can write. Ts give us static typing, and it makes our code more strict. We have to give type of the variable while declaring it, even if we use let. in functions the argument should mention its type also, and the type of the return value the function will return. This is the main goal of ts. We can even define a function's argument with what string or number or etc it will take, so that you cannot give anyother string,etc to it when you call it.  
TS never runs our code, when you run it, it will take the ts file and give a js file, the js file will run. if you run a ts file, first the esbuild command run which will generate index.js, then node index.js command runs to run the code. During compilation of ts, it will not run the code, it just inspects the file for the types, if some problem comes it will throw compilation errors, js throws runtime error. tsc is ts compiler that converts ts to js files. tsconfig file, we can tell how strict our type checking should be, it have many fields, most of them are not important to know, except some. Like target it represents the final ecmascript version of the final js, es3 and es5 are very old-internet explorer will understand, we should give ES2016-ES2020, these are good, runs in nearly all browsers. module- represents the ts file is a module or not, module is a file that exports something that can be imported. forceConsistentCasingInFileNames: true or false- will let the js file have strict casing matches to filenames for import export or not, depending on true and false. Strict: true or false, will let you not define the type of arguments or variables while declaring if set to false.  
Now we understand basic types like number, string , etc. Problem is types in real world are comples, like array of objects containing name and age of a person, there is a way to specify type more than just number and strings like collections of number and strings, we can give the type of the person in arguments of a function like (person:{name: string, age: number}) but if there will be more funcgtion then I will have to repeat it in every function, to fix this we create an interface(starts with capital letter.) interface Person{name: string, age: number}, and in the argument(person: Person) and can repeat this for other functions as well. Interface can be implemented by class, implement means that the fields defined in the interface are used by the class, and inside of the constructor we have to initialize these fields. Interface can use other interfaces, we can also extend another interface, the syntax change but does the same thing.  
Types cannot extend one another, or implemented by a class, but it is very similar to interface. It is used to define complex data, use word type instead of interface with equal to sign to assign. Types are useful for unions and ors, example we have 3 types - circle, rectangle and square, we can define type shape= rectangle | circle | square. union means and operation, replace | by &. In interface and type if we define a field like isIndian?: boolean, means that it is optional field it may exist or maynot exist. and while assigning this field in a function we will have to use this syntax let nationality: (boolean|undefined)=person.isIndian, if we don't write undefined, then it will give error. To assign array of numbers, do it using number[]. If we give argument as number[], and if we return 0th index, then we are returning a number, if we don't explicitly define our returning type, even then ts knows we are returning a number, this is called type inference. type input = (number | string)[] means an array of number or string or mix of both. but this is not a good idea, as it will return number|string, and if we want to use some function related to string only on the variable we assign its value to be, like toLowerCase(), it will give error. Hence we should use Generics, i.e, it takes function getFirstElement<T>(arr: T[]): T{return arr[0]}, T is generic, it can be string, number or anything else. If we want a function to take 2 inputs and return an array with interchanged value, the inputs can be or different types, we can do it like this function swap<T, U>(a:T, b:U):[U,T]{return [b,a]}. As it is returning array of 2 elements, we can destructure to get the values like const [val1, val2]=swap(1,"hi"); Partials in ts, make all attributes of a type or interface optional, syntax is like if we have a interface called person then type personPartial= Partial<Person>.  
Users can break our website if they send wrong inputs to the backend. Hence we should do strict input validation other wise user can send wrong input to take our backend down. Type checking for input validation using if else and try catch blocks. For example using postman  people can send json object in username, or try to fill db with dummy string that are very long length, so we should put many checks(30-40) so that our backend is safe. But this is very difficult task to identify these, so better way to do input validation is using libraries that help us with this as it is very common use case. There is a thing called process managers, if our backend goes down due to some reason, there is a process running that checks if our backend is running, if our backend goes down due to some malfunction or anything, it restarts the backend rightaway. One of the library that does this is forever and the other is pm2. Zod is a library that does the input validation for us.  
Zod lets us do input validation, what we have to do is just describe the input types that we expect user to send. It will then check any input that comes to the backend if it passes the zod type, then we use it otherwise we res.send error. We may think that zod is doing the same thing that we can do by specifing the input type in an interface and give it as the type to the user input variables, but zod is not same as ts interface checks as ts will just check the types during compile time, it will then generate a js file which will not have any of the checks so if the user send wrong input, during run time ts interface typing will not do anything as it is not present in the js file and we will get error. We define the zod input type using const which will not vanish in the js file like interface or type would, const signupInput = z.object({username: z.string().min(1).max(10), password: z.string().min(6).max(20)}) is example for the syntax.  
What are the shortcoming of react that nextjs solves. First is waterfalling, what is waterfalling, when we go to a website, first request goes out we get index.html, then we go to backend to get the user details, we render that on the screen. For example first we get a empty index.html blank screen, then comes the first render of the html that don't require the backend, then comes the signin/ signup button or your username depending on whether you are signed in or not after verifying from backend. This is called waterfalling, first the 1st backend request goes to get html then 2nd backend request goes to render more things, this is not optimal and is called waterfalling. 2nd problem is react is not SEO optimised. SE work via scraping, they see what the html file contain to find out what the website does, the initial render in react only contains html file, which is kind of empty so SE aren't able to index these websites into consideration of what they do. SE don't run js, they only see html file to read through it. 3rd React will not work where js can't run like email client for eg gmail, they only show the content, they never execute the js. SSR is we write the code in react the first render will happen on the server. So we will have a more filled initial render for SEO purposes. This is called SSR. So if we have a react app.jsx file, if we run it, we get a html file, a js and a css file, the html file have links to js file, this is not useful for SEO, email clients. If we have app.jsx file in next then if we run the file we get a full html file, the js components gets converted to html for the first render. This helps to show better content in email client and SEO. Things like useState, useEffect don't change to html, Typography from mui will change to an H1 tag, etc will happen. Next is a superset of react so we can write react code in it.  
What is SSR? The process of doing initial render on the server is SSR. All the following rendering because of state updates of variables happen on client. Can import libraries only on the server to keep small bundle size, like if there is a library that is used only in the fist SSR, then it will not be sent to the client as it is not needed there. Have complete access to backend, as theoretically all the files of backend, and db like mongodb can be stored in the same server, so before coming with the initial render next can access the mongodb db to get the username of the user as both of them are in the same server, this happens very quickly saving round trip to get backend data separately as in react.  
Facts about SSR and nextjs. No access to browser constructs like localStorage(for jwt token storage) or window, like document.getElementbyId. As the first render is hydrated/ prepared in the server side, there is no browser concept to access the local storage of browser, to access localstorage, we will have to first go the browser to get the value from browser then use in the server code. It cant run hooks, doesn't understate states. It will just hydrate the state variable value to the default value during hydration, and ignore the rest of the useState and useEffect. Also lets you write HTTP backend routes, so we don't particularly need express.  
In react we used Route for routing defined in app.jsx file, in nextjs this is not the thing, we have a folder called pages, each .tsx file in that folder is a route for that. We can see the initial html file  in the network tab of inspect, checkout the first reponse we get. Navigation between routes is different in react and next, in next we use useRouter(), router.push('/signup') syntax is used.  basically all the routing part is done by next now, no use of react-router-dom. If we use localstorage to store jwt token, server will have to make a trip to client to get the jwt token then render, not good for use. We hence have to use cookies which are always sent along with the request, otherwise there will always be waterfalling.  
on the initial render, next send react code as well as the initial html that it creates so that any statechange on button clicks, etc happen on the client side.  
We can write backend code in the same nextjs application, create a folder named api, any .ts file you write in it will become a backend route. To convert react to next, just change the navigation and routing to next routing. getServerSideProps is used to update the state variables before first render, by first getting the data and then showing that on the initial render. This can only be done by  getServerSideProps to get rid of waterfalling, also SEO will get more data as we will give more data to it in the initial render with help of getServerSideProps. useEffect, useState runs on the client side only, in order to prefill our initial state, it is done by getServerSideProps. The control first reach this function which gets the data from the backend change initial value to show the data on the first render. Avoids waterfalling and good for SEO.  

Docker- Used to setup a company's code locally and run, very easy to do. Otherwise you have to clone project, install dependencies like a language that you don't have on your pc, and so on. Hence containerization is much better way to bring things like db to your local machine, etc. Automatic restarts(like pm2?). Much more lightweight than full machines, much easier to orchestrate. Path to container orchestration. Orchestrate means create multiple instances of the code, it is not needed to understand. Why all projects have a dockerized way to set up the project locally- easy for developers(single command), and containerized code.  
Properties of a container: A container is a self contained VM(not exactly), which will have node, npm, postgres, and codebase of cal.com. I can run this code and I won't have to manually install any of the node, npm or anything, it will start a mini computer in my machine, and when I am done the mini computer closes, so there is no postgres that will take extra space on my machine as otherwise. Containerization involves building self-sufficent software packages that perform consistently regardless of the machines they run on. A developer of cal.com who have the codebase on his macbook, he can create an image of the code, by writing a file called dockerfile which specify all the dependencies needed, port numbers, etc. This image can run on any other machine like windows, ubuntu. Image is a file that contains all the files and code, etc that we need to run the application. Difference between image and container is that- When we get cal.com project locally using docker build command, it spits a large 1gb file that contains all the packages, codebase. When we run docker run command we get a website run on localhost 3000 or whatever, this thing running is called container, there can be multiple instances running of the app or containers, but there is only one Image. So when we get docker, run it on my machine it will start a mini computer which will run the application, when I am done, I kill the mini machine and all the dependencies are gone. An image running is called container.  
What is Docker- open sourse container runtime that allows software developers to build, deploy and test containerized applications on various platforms. Docker has 3 parts- Docker engine, CLI, Docker hub. Docker engine is an engine that runs when we turn on docker, it is the thing that creates images and run them. We can talk to docker instance using cli, nodejs server. We will see only cli, as it is used most of the times. Docker hub is like github of docker, docker hub is the commercial part of docker that pays their bills. Like in github, developer put their code, in dockerhub we do container registry, or put our images their. We don't push db images to docker especially in production, db images are handled by aws, etc. In docker there is only code related images not databases. So dockerhub is container registry similar to github, hosts iamges instead of code.  
How do you describe your container? How you write what it needs to run/ how it needs to run it?- Dockerfile is the answer. It is the file that describes what the image should look like, usually present at the root of the application. It is a file that describes our image, You start from a base image(ubuntu/node/alpine). Base image means that there are popular base images already out there which are OS, like ubuntu. The base image have the basic OS, and all the dependencies we want to have to run our application. Next step is add lines to install all the packages you need that are not included in the base image. Next step is copy over the files you want to present in the container(your frontend and backend code). Next is build the project(npm install npm run dev npm build). Expose the right set of ports, start your process.  
COPY . . means take every thing and copy it in workdir of docker. WORKDIR /usr/src/app means put everything in this directory. Ok so in cal.com There is a single image that have frontend and backend image, in dockerhub there is a mongo image that is taken and is run on a different container, so 2 images on 2 containers.  
How to run? Building the image- docker build -t <image_tag_name> . this . means where should we find the docker file which in our case is the current directory so ., running the image- docker run <image_tag_name>. When we run this and our application is live on localhost3000, if we open localhost3000 on my browser, nothing will show this is because docker is designed as default to not to interfere with our machine's ports. to do that we will have to run docker run -p 3000:3000 <image_tag_name> this will map our 3000 port to docker container's 3000 port. Left one is our local port, right is container's port.  
docker ps shows all the containers running(?). docker kill <image_id>. There is a general debate- why are we doing npm run install/build in the dockerfile, we can do npm install and npm run build locally and just copy the dist folder to the docker image. There is dockerignore file, it have node_modules  and dist there this means that it should not copy the node_modules and dist folder, we want to freshly create them using npm instal and npm run build command in the dockerfile. The answer is because we want enviornment specific binaries, like mac or windows, and important is optimization of layers in docker. Layers in docker are result of the way docker images are built. Each step in  a dockerfile creates a new layer thats essentially a diff of the filesystem changes since the last step. So in the dockerfile we have code, docker engine starts running each line one by one, each of these lines are building layers.  
The optimazation is done in layers, packages like node, ubuntu don't change very often they change once in a few months, so there is no need to install them freshly everytime we build the image, we can cache it. Hence we need to use layers to take advantage of the caching. For example npm install is the most time consuming in dockerbuild, if we use caching then this step is cached. There are 2 ways to get an image, either write the docker file for it or take the image from dockerhub like famous images of mongodb, ubuntu etc. For example if I run docker run -d -p 27017:27017 mongo, this will pull mongo image and run it on my machine at 27017 port. Can see it on mongocompass by connecting it to localhost:27017, it will show result. -d is argument for detach mode that is run it in the background, i.e it will free the terminal. We can deploy our images to dockerhub, like the mongo image is on dockerhub. To cache npm install first just copy package.json, if package.json is not changed( which will happen most of the times), the npm install will be cached.  
Advanced docker, how containers talk to each other and docker compose - In real world docker is used for real world deployments. Images are better than putting code to github, we just have to run 1 command to run the code someone else have written, just pull the image from dockerhub and run it. Steps to deploy on docker- Create acc on dockerhub, login using docker cli, push your image, pull the image on ec2 instance(make sure it has docker installed), run the image. Login is done using docker cli.
How to deploy backend on internet- frontend can be done via vercel, etc. we will see backend deployment. Devops/devtooling is very lucrative, we can do a lot by doing very little. Pre ci/cd days, no concept of automatic deployments, whenever you push to github, you also pull your code to the server. Deploying frontend is easy because the frontend code changes to html/css/js files and distribute them. Backend deployment is hard because every user get different response, whereas everyone gets teh same HTML/CSS/JS, like if I go to facebook I get my data from backend, while other person get his data from backend. Before deploying we should test the website functionalities are working fine. Ok so big companies like aws, azure,gcp, etc. provide servers to rent, these servers are similar to my own machine where I can run my backend, the only difference is that these servers also have a public ip which my machine don't as public ip are limited, everyone can't get a public ip. This public ip is used to connect with my domain name.  
Go to aws, search for ec2 machine, which is kind of a computer only given by aws. Select Launch instance, launch instance means get a ec2 machine. Once I launch the instance, i get a server, which have a ipv4 DNS and ipv4 address, now I can give any of them to a friend and he can see what I have hosted. High level steps to deploy the dumb way- choosing cloud provider(aws, gcp, azure). Creating an instance. Getting an ssh key(keypair, like a password to get access to server.). Opening firewalls on the machine on port 80(http)/443(https)/etc(like 3000,3001)- for example https://github.com:443 and https://github.com are same thing, http://github.com:3000 means we are connecting to 3000 port of this url. initially if I send ipv4 dns to a friend, the site will get stuck on loading because by default all the ports are closed, we have to change the security inbound rules so that these ports get opened, once they are opened the site is visitable, it won't get stuck on loading. Next step is cloning your code to the machine. For that we have to first do ssh to the server machine, for that first we have to run chmod 600 command so that aws is don't give is warning and error that other users can read our ssh key. Then we run ssh -i command to connect. Now run the git clone command to get the code to the server machine.Now we need to run npm install, but node and npm is not available in our machine so first we need to install that, then we run npm install and node index.js to run the server. Now if I shutdown my machine which I used to do ssh, the server will also shutdown, to avoid this we use process manager like pm2 that will restart the process whenever it goes down, hence the server will be running forever.  
How to update, if we have new code in the backend? ssh into machine, pull your latest code(git pull origin master), stop existing process(pm2 kill), re-build the code, re run the code(pm2 start index.js). Very manual process, we would like to put these steps in a script. make a deploy.sh file on the server machine, this file contain command for those 5 steps mentioned. Now just run source ./deploy.sh this will run the code in the file and do all the steps.  
We want to just push the code to github and the server backend updates automatically. This is done using github workflow, create .github/workflows folder, and define the workflow in this folder in a yaml file, like ci.yaml. 
